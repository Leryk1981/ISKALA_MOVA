"""
Graph Models –¥–ª—è ISKALA MOVA
–ú–æ–¥–µ–ª—å –¥–∞–Ω–∏—Ö —Ç–∞ –∞–≤—Ç–æ–≥–µ–Ω–µ—Ä–∞—Ü—ñ—è Cypher –∑–∞–ø–∏—Ç—ñ–≤
"""

from typing import Dict, List, Any, Optional, Union
from pydantic import BaseModel, Field
from datetime import datetime
from enum import Enum
import uuid
import hashlib

class NodeType(str, Enum):
    """–¢–∏–ø–∏ –≤—É–∑–ª—ñ–≤ –≤ ISKALA MOVA graph"""
    INTENT = "Intent"
    PHASE = "Phase"
    CONTEXT_CHUNK = "ContextChunk"
    USER = "User"
    SESSION = "Session"
    TOOL = "Tool"
    RESPONSE = "Response"

class RelationType(str, Enum):
    """–¢–∏–ø–∏ –∑–≤'—è–∑–∫—ñ–≤"""
    LEADS_TO = "LEADS_TO"
    CONTAINS = "CONTAINS"
    REFERENCES = "REFERENCES"
    USES = "USES"
    RESPONDS_TO = "RESPONDS_TO"
    SIMILAR_TO = "SIMILAR_TO"
    PART_OF = "PART_OF"

class BaseNode(BaseModel):
    """–ë–∞–∑–æ–≤–∞ –º–æ–¥–µ–ª—å –¥–ª—è –≤—Å—ñ—Ö –≤—É–∑–ª—ñ–≤"""
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)
    
    @staticmethod
    def merge_query(label: str, props: Dict[str, Any], unique_key: str = "id") -> str:
        """–ê–≤—Ç–æ–≥–µ–Ω–µ—Ä–∞—Ü—ñ—è MERGE –∑–∞–ø–∏—Ç—ñ–≤ –∑ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—î—é"""
        
        # –§–æ—Ä–º—É—î–º–æ WHERE —É–º–æ–≤—É –¥–ª—è —É–Ω—ñ–∫–∞–ª—å–Ω–æ–≥–æ –∫–ª—é—á–∞
        where_clause = f"{unique_key}: ${unique_key}"
        
        # –§–æ—Ä–º—É—î–º–æ SET –∫–ª–∞—É–∑—É–ª—É –¥–ª—è –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –≤–ª–∞—Å—Ç–∏–≤–æ—Å—Ç–µ–π
        set_clauses = []
        for key, value in props.items():
            if key != unique_key:  # –£–Ω—ñ–∫–∞–ª—å–Ω–∏–π –∫–ª—é—á –Ω–µ –æ–Ω–æ–≤–ª—é—î–º–æ
                set_clauses.append(f"n.{key} = ${key}")
        
        set_clause = "SET " + ", ".join(set_clauses) if set_clauses else ""
        
        query = f"""
        MERGE (n:{label} {{{where_clause}}})
        ON CREATE SET n += $props, n.created_at = datetime()
        ON MATCH SET n.updated_at = datetime()
        {set_clause}
        RETURN n
        """
        
        return query.strip()
    
    def to_cypher_params(self) -> Dict[str, Any]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è –≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–ª—è Cypher"""
        return {
            **self.dict(),
            "props": self.dict()
        }

class Intent(BaseNode):
    """–ù–∞–º—ñ—Ä –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ –≤ ISKALA MOVA"""
    name: str = Field(..., description="–ù–∞–∑–≤–∞ –Ω–∞–º—ñ—Ä—É")
    description: Optional[str] = Field(None, description="–û–ø–∏—Å –Ω–∞–º—ñ—Ä—É")
    confidence: float = Field(default=0.8, ge=0.0, le=1.0)
    lang: str = Field(default="uk", description="–ú–æ–≤–∞")
    category: Optional[str] = Field(None, description="–ö–∞—Ç–µ–≥–æ—Ä—ñ—è")
    
    # –ú–µ—Ç–∞–¥–∞–Ω—ñ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
    frequency: int = Field(default=1, description="–ß–∞—Å—Ç–æ—Ç–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è")
    success_rate: float = Field(default=0.0, ge=0.0, le=1.0)
    
    def get_unique_key(self) -> str:
        """–£–Ω—ñ–∫–∞–ª—å–Ω–∏–π –∫–ª—é—á –Ω–∞ –æ—Å–Ω–æ–≤—ñ name + lang"""
        return hashlib.md5(f"{self.name}_{self.lang}".encode()).hexdigest()

class Phase(BaseNode):
    """–§–∞–∑–∞ –º–∏—Å–ª–µ–Ω–Ω—è –≤ ISKALA MOVA"""
    name: str = Field(..., description="–ù–∞–∑–≤–∞ —Ñ–∞–∑–∏")
    order: int = Field(..., description="–ü–æ—Ä—è–¥–æ–∫ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è")
    description: Optional[str] = Field(None)
    
    # Phase-specific properties
    input_schema: Optional[Dict[str, Any]] = Field(None)
    output_schema: Optional[Dict[str, Any]] = Field(None)
    timeout_seconds: int = Field(default=30)
    
    class Config:
        # –î–æ–∑–≤–æ–ª—è—î–º–æ –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è JSON –≤ Neo4j
        json_encoders = {
            dict: lambda v: v
        }

class ContextChunk(BaseNode):
    """üß† Enhanced ContextChunk with vector embeddings for semantic search"""
    content: str = Field(..., description="–¢–µ–∫—Å—Ç–æ–≤–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç")
    source_doc: str = Field(..., description="–ù–∞–∑–≤–∞ –¥–∂–µ—Ä–µ–ª—å–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
    chunk_hash: str = Field(..., description="–•–µ—à –¥–ª—è –¥–µ–¥—É–ø–ª—ñ–∫–∞—Ü—ñ—ó")
    
    # üß† Vector embeddings (384-dimensional for sentence-transformers)
    embedding: List[float] = Field(default_factory=list, description="384-dim embedding vector")
    
    # üåç Multilingual support
    language: str = Field(default="uk", description="ISO 639-1 language code")
    
    # üìä Processing metadata from MultilingualDocumentProcessor
    position: int = Field(default=0, description="Chunk position in document")
    confidence: float = Field(default=1.0, ge=0.0, le=1.0, description="Processing confidence")
    word_count: int = Field(default=0, ge=0)
    sentence_count: int = Field(default=0, ge=0)
    
    # üîç Search optimization
    keywords: List[str] = Field(default_factory=list)
    metadata: Dict[str, Any] = Field(default_factory=dict)
    
    @classmethod
    def create_from_text(cls, text: str, source: str) -> "ContextChunk":
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è ContextChunk –∑ —Ç–µ–∫—Å—Ç—É"""
        chunk_hash = hashlib.sha256(text.encode()).hexdigest()
        return cls(
            content=text,
            source_doc=source,
            chunk_hash=chunk_hash
        )
    
    @classmethod
    def from_doc_chunk(cls, doc_chunk, embedding: List[float]) -> "ContextChunk":
        """Create ContextChunk from MultilingualDocumentProcessor DocChunk + embedding"""
        return cls(
            content=doc_chunk.content,
            source_doc=doc_chunk.source_doc,
            chunk_hash=doc_chunk.chunk_hash,
            embedding=embedding,
            language=doc_chunk.language,
            position=doc_chunk.position,
            confidence=doc_chunk.confidence,
            word_count=doc_chunk.word_count,
            sentence_count=doc_chunk.sentence_count,
            metadata=doc_chunk.metadata
        )
    
    def validate_embedding_dimensions(self) -> bool:
        """Validate that embedding has correct dimensions (384 for sentence-transformers)"""
        return len(self.embedding) == 384 if self.embedding else True
    
    def to_cypher_merge(self) -> str:
        """Generate optimized Cypher MERGE query with vector support"""
        return f"""
        MERGE (c:ContextChunk {{chunk_hash: '{self.chunk_hash}'}})
        SET c.content = $content,
            c.source_doc = $source_doc,
            c.language = $language,
            c.embedding = $embedding,
            c.position = $position,
            c.confidence = $confidence,
            c.word_count = $word_count,
            c.sentence_count = $sentence_count,
            c.keywords = $keywords,
            c.metadata = $metadata,
            c.updated_at = datetime()
        ON CREATE SET c.created_at = datetime()
        RETURN c
        """

class User(BaseNode):
    """–ö–æ—Ä–∏—Å—Ç—É–≤–∞—á —Å–∏—Å—Ç–µ–º–∏"""
    username: str = Field(..., description="–Ü–º'—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞")
    email: Optional[str] = Field(None)
    preferences: Dict[str, Any] = Field(default_factory=dict)
    
    def get_unique_key(self) -> str:
        return self.username

class Session(BaseNode):
    """–°–µ—Å—ñ—è –≤–∑–∞—î–º–æ–¥—ñ—ó"""
    user_id: str = Field(..., description="ID –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞")
    session_data: Dict[str, Any] = Field(default_factory=dict)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–µ—Å—ñ—ó
    queries_count: int = Field(default=0)
    start_time: datetime = Field(default_factory=datetime.utcnow)
    last_activity: datetime = Field(default_factory=datetime.utcnow)

class GraphQueryBuilder:
    """–ë—É–¥—ñ–≤–Ω–∏–∫ —Å–∫–ª–∞–¥–Ω–∏—Ö Cypher –∑–∞–ø–∏—Ç—ñ–≤"""
    
    @staticmethod
    def create_intent_tree_query(root_intent: str) -> str:
        """–ó–∞–ø–∏—Ç –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –¥–µ—Ä–µ–≤–∞ –Ω–∞–º—ñ—Ä—ñ–≤"""
        return """
        // –ó–Ω–∞—Ö–æ–¥–∏–º–æ –∫–æ—Ä–Ω–µ–≤–∏–π –Ω–∞–º—ñ—Ä
        MATCH (root:Intent {name: $root_intent})
        
        // –°—Ç–≤–æ—Ä—é—î–º–æ –≥—Ä–∞—Ñ –ø–æ–≤'—è–∑–∞–Ω–∏—Ö –Ω–∞–º—ñ—Ä—ñ–≤
        OPTIONAL MATCH (root)-[:LEADS_TO*1..3]->(child:Intent)
        
        // –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–µ—Ä–µ–≤–∞
        RETURN root, 
               collect(DISTINCT child) as children,
               [p = (root)-[:LEADS_TO*1..3]->(child) | p] as paths
        """
    
    @staticmethod
    def rag_context_search_query(intent_name: str, limit: int = 5) -> str:
        """–ü–æ—à—É–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –¥–ª—è –Ω–∞–º—ñ—Ä—É"""
        return """
        // –ó–Ω–∞—Ö–æ–¥–∏–º–æ –Ω–∞–º—ñ—Ä
        MATCH (i:Intent {name: $intent_name})
        
        // –®—É–∫–∞—î–º–æ –ø–æ–≤'—è–∑–∞–Ω–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        MATCH (i)-[:REFERENCES]->(chunk:ContextChunk)
        
        // –ê–±–æ —à—É–∫–∞—î–º–æ —Å—Ö–æ–∂–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∑–∞ –∫–ª—é—á–æ–≤–∏–º–∏ —Å–ª–æ–≤–∞–º–∏
        OPTIONAL MATCH (similar:ContextChunk)
        WHERE any(keyword IN similar.keywords WHERE keyword IN i.keywords)
        
        // –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –Ω–∞–π—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—à—ñ —á–∞–Ω–∫–∏
        WITH i, chunk, similar, 
             CASE WHEN chunk IS NOT NULL THEN 1.0 ELSE 0.5 END as relevance_score
        
        RETURN DISTINCT 
               coalesce(chunk, similar) as context_chunk,
               relevance_score
        ORDER BY relevance_score DESC
        LIMIT $limit
        """
    
    @staticmethod
    def user_session_analysis_query(user_id: str) -> str:
        """–ê–Ω–∞–ª—ñ–∑ —Å–µ—Å—ñ–π –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞"""
        return """
        MATCH (u:User {id: $user_id})-[:HAS_SESSION]->(s:Session)
        MATCH (s)-[:CONTAINS]->(i:Intent)
        
        WITH u, s, i, 
             count(i) as intent_count,
             collect(DISTINCT i.category) as categories
        
        RETURN u.username as username,
               s.id as session_id,
               s.start_time as session_start,
               intent_count,
               categories,
               s.queries_count as total_queries
        ORDER BY s.start_time DESC
        """

class CypherTemplates:
    """–ö–æ–ª–µ–∫—Ü—ñ—è –≥–æ—Ç–æ–≤–∏—Ö Cypher –∑–∞–ø–∏—Ç—ñ–≤"""
    
    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —ñ–Ω–¥–µ–∫—Å—ñ–≤
    CREATE_INDEXES = [
        "CREATE INDEX intent_name_idx IF NOT EXISTS FOR (i:Intent) ON (i.name)",
        "CREATE INDEX intent_category_idx IF NOT EXISTS FOR (i:Intent) ON (i.category)",
        "CREATE INDEX chunk_hash_idx IF NOT EXISTS FOR (c:ContextChunk) ON (c.chunk_hash)",
        "CREATE INDEX user_username_idx IF NOT EXISTS FOR (u:User) ON (u.username)",
        "CREATE INDEX session_user_idx IF NOT EXISTS FOR (s:Session) ON (s.user_id)",
        
        # Vector index –¥–ª—è embeddings (–ø–æ—Ç—Ä–µ–±—É—î Neo4j 5.x)
        """CREATE VECTOR INDEX chunk_embedding_idx IF NOT EXISTS
           FOR (c:ContextChunk) ON (c.embedding)
           OPTIONS {indexConfig: {
             `vector.dimensions`: 384,
             `vector.similarity_function`: 'cosine'
           }}"""
    ]
    
    # Constraints
    CREATE_CONSTRAINTS = [
        "CREATE CONSTRAINT intent_name_unique IF NOT EXISTS FOR (i:Intent) ON (i.name, i.lang)",
        "CREATE CONSTRAINT chunk_hash_unique IF NOT EXISTS FOR (c:ContextChunk) ON (c.chunk_hash)",
        "CREATE CONSTRAINT user_username_unique IF NOT EXISTS FOR (u:User) ON (u.username)"
    ]
    
    @staticmethod
    def vector_similarity_query(embedding: List[float], top_k: int = 10) -> str:
        """–ü–æ—à—É–∫ —Å—Ö–æ–∂–∏—Ö ContextChunk –∑–∞ embedding"""
        return f"""
        CALL db.index.vector.queryNodes('chunk_embedding_idx', {top_k}, $embedding)
        YIELD node as chunk, score
        RETURN chunk.content as content,
               chunk.source as source,
               chunk.keywords as keywords,
               score
        ORDER BY score DESC
        """ 