"""
Embedding Service –¥–ª—è ISKALA MOVA
=================================

Production-ready —Å–µ—Ä–≤—ñ—Å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó —Ç–∞ –∫–µ—à—É–≤–∞–Ω–Ω—è embeddings –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º:
- sentence-transformers (all-MiniLM-L6-v2) - –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π –¥–ª—è —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—ó –º–æ–≤–∏
- Redis –∫–µ—à—É–≤–∞–Ω–Ω—è –∑ zstd –∫–æ–º–ø—Ä–µ—Å—ñ—î—é
- Async/await –ø—ñ–¥—Ç—Ä–∏–º–∫–∞ –∑ batch processing
- Multi-GPU –ø—ñ–¥—Ç—Ä–∏–º–∫–∞ –¥–ª—è production –Ω–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
"""

import os
import asyncio
import hashlib
import json
import logging
import time
from typing import List, Dict, Any, Optional, Union
from functools import wraps
import numpy as np

import redis.asyncio as redis
import zstd
from sentence_transformers import SentenceTransformer, util
import torch
from pydantic import BaseModel, Field

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ª–æ–≥—É–≤–∞–Ω–Ω—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EmbeddingConfig(BaseModel):
    """–ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –¥–ª—è Embedding Service"""
    model_name: str = Field(default="all-MiniLM-L6-v2", description="Sentence-transformers –º–æ–¥–µ–ª—å")
    device: str = Field(default="auto", description="–û–±—á–∏—Å–ª—é–≤–∞–ª—å–Ω–∏–π –ø—Ä–∏—Å—Ç—Ä—ñ–π")
    normalize_embeddings: bool = Field(default=True, description="–ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –¥–ª—è dot-product")
    
    # Redis –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è
    redis_host: str = Field(default="localhost")
    redis_port: int = Field(default=6379)
    redis_password: Optional[str] = Field(default=None)
    cache_ttl: int = Field(default=3600, description="TTL –≤ —Å–µ–∫—É–Ω–¥–∞—Ö")
    use_compression: bool = Field(default=True, description="zstd –∫–æ–º–ø—Ä–µ—Å—ñ—è")
    
    # Performance –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è
    batch_size: int = Field(default=32, description="–†–æ–∑–º—ñ—Ä batch –¥–ª—è encode")
    max_seq_length: int = Field(default=512, description="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ")
    
    # Multi-processing
    multi_process_devices: Optional[List[str]] = Field(default=None, description="–°–ø–∏—Å–æ–∫ –ø—Ä–∏—Å—Ç—Ä–æ—ó–≤ –¥–ª—è multi-process")

class EmbeddingStats(BaseModel):
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–æ–±–æ—Ç–∏ Embedding Service"""
    total_requests: int = 0
    cache_hits: int = 0
    cache_misses: int = 0
    total_tokens_processed: int = 0
    avg_processing_time_ms: float = 0.0
    
    @property
    def hit_rate(self) -> float:
        """–í—ñ–¥—Å–æ—Ç–æ–∫ –ø–æ–ø–∞–¥–∞–Ω—å —É –∫–µ—à"""
        if self.total_requests == 0:
            return 0.0
        return (self.cache_hits / self.total_requests) * 100

def timing_decorator(func):
    """–î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è –≤–∏–º—ñ—Ä—é–≤–∞–Ω–Ω—è —á–∞—Å—É –≤–∏–∫–æ–Ω–∞–Ω–Ω—è"""
    @wraps(func)
    async def wrapper(*args, **kwargs):
        start_time = time.time()
        result = await func(*args, **kwargs)
        end_time = time.time()
        duration_ms = (end_time - start_time) * 1000
        logger.debug(f"{func.__name__} took {duration_ms:.2f}ms")
        return result
    return wrapper

class EmbeddingService:
    """
    Production-ready Embedding Service –¥–ª—è ISKALA MOVA
    
    –û—Å–æ–±–ª–∏–≤–æ—Å—Ç—ñ:
    - Async/await API
    - Redis –∫–µ—à—É–≤–∞–Ω–Ω—è –∑ –∫–æ–º–ø—Ä–µ—Å—ñ—î—é
    - Batch processing –¥–ª—è –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó
    - Multi-GPU –ø—ñ–¥—Ç—Ä–∏–º–∫–∞
    - –î–µ—Ç–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–∞ –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥
    """
    
    def __init__(self, config: Optional[EmbeddingConfig] = None):
        self.config = config or EmbeddingConfig()
        self.model: Optional[SentenceTransformer] = None
        self.redis_client: Optional[redis.Redis] = None
        self.multi_process_pool = None
        self.stats = EmbeddingStats()
        self._initialized = False
        
        logger.info(f"EmbeddingService —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ –∑ –º–æ–¥–µ–ª–ª—é: {self.config.model_name}")
    
    async def initialize(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —Å–µ—Ä–≤—ñ—Å—É"""
        if self._initialized:
            return
        
        try:
            # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è sentence-transformers –º–æ–¥–µ–ª—ñ
            await self._init_model()
            
            # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è Redis –∫–ª—ñ—î–Ω—Ç–∞
            await self._init_redis()
            
            self._initialized = True
            logger.info("‚úÖ EmbeddingService —É—Å–ø—ñ—à–Ω–æ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ")
            
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó EmbeddingService: {e}")
            raise
    
    async def _init_model(self):
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è sentence-transformers –º–æ–¥–µ–ª—ñ"""
        # –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è –ø—Ä–∏—Å—Ç—Ä–æ—é
        if self.config.device == "auto":
            if torch.cuda.is_available():
                device = "cuda"
                logger.info(f"üöÄ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è CUDA: {torch.cuda.get_device_name()}")
            elif torch.backends.mps.is_available():
                device = "mps"
                logger.info("üöÄ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è Apple MPS")
            else:
                device = "cpu"
                logger.info("üíª –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è CPU")
        else:
            device = self.config.device
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
        self.model = SentenceTransformer(
            self.config.model_name,
            device=device
        )
        
        # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ—ó –¥–æ–≤–∂–∏–Ω–∏ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ
        if hasattr(self.model, 'max_seq_length'):
            self.model.max_seq_length = self.config.max_seq_length
        
        # –û–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–µ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è multi-process pool
        if self.config.multi_process_devices:
            self.multi_process_pool = self.model.start_multi_process_pool(
                devices=self.config.multi_process_devices
            )
            logger.info(f"üîÑ Multi-process pool —Å—Ç–≤–æ—Ä–µ–Ω–æ –∑ –ø—Ä–∏—Å—Ç—Ä–æ—è–º–∏: {self.config.multi_process_devices}")
        
        logger.info(f"üìä –ú–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ –Ω–∞ –ø—Ä–∏—Å—Ç—Ä—ñ–π: {device}")
        logger.info(f"üìè –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ: {self.config.max_seq_length}")
        logger.info(f"üìê –†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å embedding: {self.model.get_sentence_embedding_dimension()}")
    
    async def _init_redis(self):
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è Redis –∫–ª—ñ—î–Ω—Ç–∞"""
        try:
            self.redis_client = redis.Redis(
                host=self.config.redis_host,
                port=self.config.redis_port,
                password=self.config.redis_password,
                decode_responses=False,  # –î–ª—è binary data
                socket_connect_timeout=5,
                socket_timeout=5,
                health_check_interval=30
            )
            
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∑'—î–¥–Ω–∞–Ω–Ω—è
            await self.redis_client.ping()
            logger.info("‚úÖ Redis –∑'—î–¥–Ω–∞–Ω–Ω—è –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Redis –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π, –ø—Ä–æ–¥–æ–≤–∂—É—î–º–æ –±–µ–∑ –∫–µ—à—É–≤–∞–Ω–Ω—è: {e}")
            self.redis_client = None
    
    def _generate_cache_key(self, text: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∫–ª—é—á–∞ –∫–µ—à—É –Ω–∞ –æ—Å–Ω–æ–≤—ñ SHA256 —Ö–µ—à—É —Ç–µ–∫—Å—Ç—É"""
        text_hash = hashlib.sha256(text.encode('utf-8')).hexdigest()
        model_version = self.config.model_name.replace('/', '_')
        max_len = self.config.max_seq_length
        return f"emb:{model_version}:{max_len}:{text_hash}"
    
    async def _get_from_cache(self, cache_key: str) -> Optional[List[float]]:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è embedding –∑ –∫–µ—à—É"""
        if not self.redis_client:
            return None
        
        try:
            cached_data = await self.redis_client.get(cache_key)
            if cached_data:
                if self.config.use_compression:
                    # –î–µ–∫–æ–º–ø—Ä–µ—Å—ñ—è zstd
                    decompressed_data = zstd.decompress(cached_data)
                    embedding = json.loads(decompressed_data.decode('utf-8'))
                else:
                    embedding = json.loads(cached_data.decode('utf-8'))
                
                self.stats.cache_hits += 1
                logger.debug(f"Cache hit for key: {cache_key[:16]}...")
                return embedding
                
        except Exception as e:
            logger.warning(f"–ü–æ–º–∏–ª–∫–∞ —á–∏—Ç–∞–Ω–Ω—è –∑ –∫–µ—à—É: {e}")
        
        self.stats.cache_misses += 1
        return None
    
    async def _save_to_cache(self, cache_key: str, embedding: List[float]):
        """–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è embedding –≤ –∫–µ—à"""
        if not self.redis_client:
            return
        
        try:
            embedding_json = json.dumps(embedding).encode('utf-8')
            
            if self.config.use_compression:
                # –ö–æ–º–ø—Ä–µ—Å—ñ—è zstd
                compressed_data = zstd.compress(embedding_json, level=3)
                data_to_save = compressed_data
            else:
                data_to_save = embedding_json
            
            await self.redis_client.set(
                cache_key,
                data_to_save,
                ex=self.config.cache_ttl
            )
            
            logger.debug(f"–ó–±–µ—Ä–µ–∂–µ–Ω–æ –≤ –∫–µ—à: {cache_key[:16]}...")
            
        except Exception as e:
            logger.warning(f"–ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤ –∫–µ—à: {e}")
    
    @timing_decorator
    async def get_embedding(self, text: str) -> List[float]:
        """
        –û—Ç—Ä–∏–º–∞–Ω–Ω—è embedding –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É
        
        Args:
            text: –í—Ö—ñ–¥–Ω–∏–π —Ç–µ–∫—Å—Ç –¥–ª—è encoding
            
        Returns:
            –°–ø–∏—Å–æ–∫ float –∑–Ω–∞—á–µ–Ω—å embedding
        """
        if not self._initialized:
            await self.initialize()
        
        self.stats.total_requests += 1
        
        # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∫–ª—é—á–∞ –∫–µ—à—É
        cache_key = self._generate_cache_key(text)
        
        # –°–ø—Ä–æ–±–∞ –æ—Ç—Ä–∏–º–∞—Ç–∏ –∑ –∫–µ—à—É
        cached_embedding = await self._get_from_cache(cache_key)
        if cached_embedding:
            return cached_embedding
        
        # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –Ω–æ–≤–æ–≥–æ embedding
        start_time = time.time()
        
        if self.multi_process_pool:
            embeddings = self.model.encode([text], pool=self.multi_process_pool)
        else:
            embeddings = self.model.encode([text])
        
        # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –¥–ª—è –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó dot-product
        if self.config.normalize_embeddings:
            embeddings = util.normalize_embeddings(embeddings)
        
        embedding = embeddings[0].tolist()
        
        # –û–Ω–æ–≤–ª–µ–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        processing_time = (time.time() - start_time) * 1000
        self.stats.total_tokens_processed += len(text.split())
        self._update_avg_processing_time(processing_time)
        
        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤ –∫–µ—à
        await self._save_to_cache(cache_key, embedding)
        
        return embedding
    
    @timing_decorator
    async def get_embeddings_batch(self, texts: List[str]) -> List[List[float]]:
        """
        Batch encoding –¥–ª—è —Å–ø–∏—Å–∫—É —Ç–µ–∫—Å—Ç—ñ–≤ (–æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–æ –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ)
        
        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç—ñ–≤ –¥–ª—è encoding
            
        Returns:
            –°–ø–∏—Å–æ–∫ embedding –≤–µ–∫—Ç–æ—Ä—ñ–≤
        """
        if not self._initialized:
            await self.initialize()
        
        if not texts:
            return []
        
        self.stats.total_requests += len(texts)
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∫–µ—à—É –¥–ª—è –≤—Å—ñ—Ö —Ç–µ–∫—Å—Ç—ñ–≤
        cache_keys = [self._generate_cache_key(text) for text in texts]
        cached_results = {}
        texts_to_process = []
        indices_to_process = []
        
        for i, (text, cache_key) in enumerate(zip(texts, cache_keys)):
            cached_embedding = await self._get_from_cache(cache_key)
            if cached_embedding:
                cached_results[i] = cached_embedding
            else:
                texts_to_process.append(text)
                indices_to_process.append(i)
        
        logger.info(f"Batch processing: {len(cached_results)} –∑ –∫–µ—à—É, {len(texts_to_process)} –Ω–æ–≤–∏—Ö")
        
        # Batch encoding –¥–ª—è –Ω–æ–≤–∏—Ö —Ç–µ–∫—Å—Ç—ñ–≤
        new_embeddings = []
        if texts_to_process:
            start_time = time.time()
            
            if self.multi_process_pool:
                embeddings = self.model.encode(texts_to_process, pool=self.multi_process_pool)
            else:
                embeddings = self.model.encode(
                    texts_to_process,
                    batch_size=self.config.batch_size,
                    show_progress_bar=len(texts_to_process) > 100
                )
            
            # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è
            if self.config.normalize_embeddings:
                embeddings = util.normalize_embeddings(embeddings)
            
            new_embeddings = [emb.tolist() for emb in embeddings]
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            processing_time = (time.time() - start_time) * 1000
            total_tokens = sum(len(text.split()) for text in texts_to_process)
            self.stats.total_tokens_processed += total_tokens
            self._update_avg_processing_time(processing_time)
            
            # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –Ω–æ–≤–∏—Ö embedding –≤ –∫–µ—à
            for i, (text, embedding) in enumerate(zip(texts_to_process, new_embeddings)):
                cache_key = self._generate_cache_key(text)
                await self._save_to_cache(cache_key, embedding)
        
        # –ó–±–∏—Ä–∞–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ —É –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É –ø–æ—Ä—è–¥–∫—É
        results = [None] * len(texts)
        
        # –î–æ–¥–∞—î–º–æ –∫–µ—à–æ–≤–∞–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
        for i, embedding in cached_results.items():
            results[i] = embedding
        
        # –î–æ–¥–∞—î–º–æ –Ω–æ–≤—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
        for i, embedding_idx in enumerate(indices_to_process):
            results[embedding_idx] = new_embeddings[i]
        
        return results
    
    def _update_avg_processing_time(self, new_time_ms: float):
        """–û–Ω–æ–≤–ª–µ–Ω–Ω—è —Å–µ—Ä–µ–¥–Ω—å–æ–≥–æ —á–∞—Å—É –æ–±—Ä–æ–±–∫–∏"""
        if self.stats.avg_processing_time_ms == 0:
            self.stats.avg_processing_time_ms = new_time_ms
        else:
            # –ï–∫—Å–ø–æ–Ω–µ–Ω—Ü—ñ–∞–ª—å–Ω–µ –∑–≥–ª–∞–¥–∂—É–≤–∞–Ω–Ω—è
            alpha = 0.1
            self.stats.avg_processing_time_ms = (
                alpha * new_time_ms + (1 - alpha) * self.stats.avg_processing_time_ms
            )
    
    async def get_similarity(self, text1: str, text2: str) -> float:
        """–û–±—á–∏—Å–ª–µ–Ω–Ω—è cosine similarity –º—ñ–∂ –¥–≤–æ–º–∞ —Ç–µ–∫—Å—Ç–∞–º–∏"""
        embeddings = await self.get_embeddings_batch([text1, text2])
        
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ util.cos_sim –∑ sentence-transformers
        similarity = util.cos_sim(embeddings[0], embeddings[1]).item()
        return similarity
    
    async def find_most_similar(
        self, 
        query: str, 
        candidates: List[str], 
        top_k: int = 5
    ) -> List[Dict[str, Any]]:
        """–ü–æ—à—É–∫ –Ω–∞–π–±—ñ–ª—å—à —Å—Ö–æ–∂–∏—Ö —Ç–µ–∫—Å—Ç—ñ–≤ –∑ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞–º–∏"""
        
        # –û—Ç—Ä–∏–º—É—î–º–æ embeddings
        all_texts = [query] + candidates
        embeddings = await self.get_embeddings_batch(all_texts)
        
        query_embedding = embeddings[0]
        candidate_embeddings = embeddings[1:]
        
        # –û–±—á–∏—Å–ª—é—î–º–æ similarities
        similarities = util.cos_sim([query_embedding], candidate_embeddings)[0]
        
        # –°–æ—Ä—Ç—É—î–º–æ —Ç–∞ –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ top_k
        results = []
        for i, sim_score in enumerate(similarities):
            results.append({
                "text": candidates[i],
                "similarity": sim_score.item(),
                "index": i
            })
        
        # –°–æ—Ä—Ç—É–≤–∞–Ω–Ω—è –∑–∞ —Å–ø–∞–¥–∞–Ω–Ω—è–º similarity
        results.sort(key=lambda x: x["similarity"], reverse=True)
        
        return results[:top_k]
    
    async def get_stats(self) -> Dict[str, Any]:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ä–æ–±–æ—Ç–∏ —Å–µ—Ä–≤—ñ—Å—É"""
        redis_info = {}
        if self.redis_client:
            try:
                redis_info = await self.redis_client.info("memory")
                redis_info = {
                    "used_memory_human": redis_info.get("used_memory_human", "N/A"),
                    "used_memory_peak_human": redis_info.get("used_memory_peak_human", "N/A")
                }
            except:
                redis_info = {"status": "error"}
        
        return {
            "model_info": {
                "name": self.config.model_name,
                "device": str(self.model.device) if self.model else "not_loaded",
                "dimension": self.model.get_sentence_embedding_dimension() if self.model else 0,
                "max_seq_length": self.config.max_seq_length
            },
            "performance": {
                "total_requests": self.stats.total_requests,
                "cache_hit_rate": f"{self.stats.hit_rate:.2f}%",
                "total_tokens_processed": self.stats.total_tokens_processed,
                "avg_processing_time_ms": f"{self.stats.avg_processing_time_ms:.2f}"
            },
            "cache": {
                "enabled": self.redis_client is not None,
                "compression": self.config.use_compression,
                "ttl_seconds": self.config.cache_ttl,
                "redis_info": redis_info
            },
            "multi_process": {
                "enabled": self.multi_process_pool is not None,
                "devices": self.config.multi_process_devices
            }
        }
    
    async def health_check(self) -> Dict[str, Any]:
        """Health check –¥–ª—è –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É"""
        health = {
            "service": "embedding_service",
            "status": "healthy",
            "model_loaded": self.model is not None,
            "redis_connected": False,
            "timestamp": time.time()
        }
        
        if self.redis_client:
            try:
                await self.redis_client.ping()
                health["redis_connected"] = True
            except:
                health["redis_connected"] = False
        
        # –¢–µ—Å—Ç –ø—Ä–æ—Å—Ç–æ–≥–æ encoding
        try:
            test_embedding = await self.get_embedding("test")
            health["embedding_test"] = len(test_embedding) > 0
        except Exception as e:
            health["status"] = "unhealthy"
            health["error"] = str(e)
        
        return health
    
    async def clear_cache(self, pattern: str = "emb:*") -> int:
        """–û—á–∏—â–µ–Ω–Ω—è –∫–µ—à—É –∑–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–º"""
        if not self.redis_client:
            return 0
        
        try:
            keys = await self.redis_client.keys(pattern)
            if keys:
                deleted = await self.redis_client.delete(*keys)
                logger.info(f"–í–∏–¥–∞–ª–µ–Ω–æ {deleted} –∫–ª—é—á—ñ–≤ –∑ –∫–µ—à—É")
                return deleted
            return 0
        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –æ—á–∏—â–µ–Ω–Ω—è –∫–µ—à—É: {e}")
            return 0
    
    async def close(self):
        """–ó–∞–∫—Ä–∏—Ç—Ç—è –≤—Å—ñ—Ö –∑'—î–¥–Ω–∞–Ω—å —Ç–∞ —Ä–µ—Å—É—Ä—Å—ñ–≤"""
        if self.multi_process_pool:
            self.model.stop_multi_process_pool(self.multi_process_pool)
            logger.info("Multi-process pool –∑—É–ø–∏–Ω–µ–Ω–æ")
        
        if self.redis_client:
            await self.redis_client.close()
            logger.info("Redis –∑'—î–¥–Ω–∞–Ω–Ω—è –∑–∞–∫—Ä–∏—Ç–æ")
        
        self._initialized = False
        logger.info("EmbeddingService –∑–∞–∫—Ä–∏—Ç–æ")

# –ì–ª–æ–±–∞–ª—å–Ω–∏–π instance –¥–ª—è reuse
_embedding_service: Optional[EmbeddingService] = None

async def get_embedding_service(config: Optional[EmbeddingConfig] = None) -> EmbeddingService:
    """–û—Ç—Ä–∏–º–∞–Ω–Ω—è –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ instance EmbeddingService"""
    global _embedding_service
    
    if not _embedding_service:
        # –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –∑ environment variables
        env_config = EmbeddingConfig(
            model_name=os.getenv("EMBEDDING_MODEL", "all-MiniLM-L6-v2"),
            redis_host=os.getenv("REDIS_HOST", "localhost"),
            redis_port=int(os.getenv("REDIS_PORT", "6379")),
            redis_password=os.getenv("REDIS_PASSWORD"),
            cache_ttl=int(os.getenv("EMBEDDING_CACHE_TTL", "3600"))
        )
        
        _embedding_service = EmbeddingService(config or env_config)
        await _embedding_service.initialize()
    
    return _embedding_service

async def close_embedding_service():
    """–ó–∞–∫—Ä–∏—Ç—Ç—è –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —Å–µ—Ä–≤—ñ—Å—É"""
    global _embedding_service
    if _embedding_service:
        await _embedding_service.close()
        _embedding_service = None 