#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸŒ ISKALA MOVA Multilingual Document Processing Demo
Ğ”ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ñ–Ñ Ğ±Ğ°Ğ³Ğ°Ñ‚Ğ¾Ğ¼Ğ¾Ğ²Ğ½Ğ¾Ñ— Ğ¾Ğ±Ñ€Ğ¾Ğ±ĞºĞ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ–Ğ² Ğ´Ğ»Ñ ISKALA MOVA
å¤šè¯­è¨€æ–‡æ¡£å¤„ç†æ¼”ç¤º

This demo showcases the enterprise-grade multilingual document chunking system.
"""

import asyncio
import sys
import time
from pathlib import Path
from typing import Dict, List

# Add parent directory for imports
sys.path.append(str(Path(__file__).parent.parent))

from services.document_processor import (
    MultilingualDocumentProcessor,
    DocChunk,
    LanguageCode,
    process_multilingual_document,
    chunk_multilingual_text
)

class MultilingualDemo:
    """
    ğŸŒ Comprehensive demo of multilingual document processing capabilities
    """
    
    def __init__(self):
        self.processor = MultilingualDocumentProcessor(
            chunk_size=400,
            chunk_overlap=100,
            auto_detect_language=True
        )
        
    async def demo_language_detection(self):
        """Demo automatic language detection"""
        print("\nğŸ” LANGUAGE DETECTION DEMO")
        print("=" * 50)
        
        test_texts = {
            "English": "Artificial Intelligence has revolutionized how we process natural language. Machine Learning algorithms can understand human text with remarkable accuracy.",
            
            "Ukrainian": "Ğ¨Ñ‚ÑƒÑ‡Ğ½Ğ¸Ğ¹ Ñ–Ğ½Ñ‚ĞµĞ»ĞµĞºÑ‚ Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ñ–Ğ¾Ğ½Ñ–Ğ·ÑƒĞ²Ğ°Ğ² Ğ¾Ğ±Ñ€Ğ¾Ğ±ĞºÑƒ Ğ¿Ñ€Ğ¸Ñ€Ğ¾Ğ´Ğ½Ğ¾Ñ— Ğ¼Ğ¾Ğ²Ğ¸. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ²Ñ‡Ğ°Ğ½Ğ½Ñ Ğ¼Ğ¾Ğ¶ÑƒÑ‚ÑŒ Ñ€Ğ¾Ğ·ÑƒĞ¼Ñ–Ñ‚Ğ¸ Ğ»ÑĞ´ÑÑŒĞºĞ¸Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ğ· Ğ½Ğ°Ğ´Ğ·Ğ²Ğ¸Ñ‡Ğ°Ğ¹Ğ½Ğ¾Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ–ÑÑ‚Ñ.",
            
            "Chinese": "äººå·¥æ™ºèƒ½å½»åº•æ”¹å˜äº†æˆ‘ä»¬å¤„ç†è‡ªç„¶è¯­è¨€çš„æ–¹å¼ã€‚æœºå™¨å­¦ä¹ ç®—æ³•èƒ½å¤Ÿä»¥æƒŠäººçš„å‡†ç¡®æ€§ç†è§£äººç±»æ–‡æœ¬ã€‚",
            
            "Russian": "Ğ˜ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ» Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ñ ÑƒĞ´Ğ¸Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ.",
            
            "Spanish": "La Inteligencia Artificial ha revolucionado cÃ³mo procesamos el lenguaje natural. Los algoritmos de Machine Learning pueden entender texto humano con precisiÃ³n notable.",
            
            "Mixed (Code)": """
            # English comment: Initialize AI model
            def initialize_model():
                # Ğ£ĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ¸Ğ¹ ĞºĞ¾Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€: Ñ–Ğ½Ñ–Ñ†Ñ–Ğ°Ğ»Ñ–Ğ·Ğ°Ñ†Ñ–Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ñ–
                model = "gpt-4"  
                # ä¸­æ–‡æ³¨é‡Šï¼šæ¨¡å‹é…ç½®
                return model
            """
        }
        
        for name, text in test_texts.items():
            print(f"\nğŸ“ {name} Text Sample:")
            print(f"   {text[:80]}...")
            
            chunks = await self.processor.process_text(text, f"demo_{name.lower()}.txt")
            
            if chunks:
                detected_lang = chunks[0].language
                confidence = chunks[0].confidence
                print(f"ğŸŒ Detected Language: {detected_lang}")
                print(f"ğŸ“Š Confidence: {confidence:.3f}")
                print(f"ğŸ”¢ Chunks Created: {len(chunks)}")
            else:
                print("âŒ No chunks created")

    async def demo_tokenizer_features(self):
        """Demo language-specific tokenization features"""
        print("\nğŸ”§ TOKENIZER FEATURES DEMO")
        print("=" * 50)
        
        # Ukrainian compound terms preservation
        ukrainian_text = """
        Ğ”ĞµÑ€Ğ¶Ğ°Ğ²Ğ½Ğ¾-Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğµ Ğ¿Ğ°Ñ€Ñ‚Ğ½ĞµÑ€ÑÑ‚Ğ²Ğ¾ Ğ² Ñ–Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ñ–Ğ¹Ğ½Ğ¾-ĞºĞ¾Ğ¼ÑƒĞ½Ñ–ĞºĞ°Ñ†Ñ–Ğ¹Ğ½Ğ¸Ñ… Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ñ–ÑÑ… 
        Ğ¿Ğ¾ĞºĞ°Ğ·ÑƒÑ” Ğ²Ñ–Ğ´Ğ¼Ñ–Ğ½Ğ½Ñ– Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¸. Ğ¢Ğ°Ñ€Ğ°Ñ Ğ¨ĞµĞ²Ñ‡ĞµĞ½ĞºĞ¾ Ñ‚Ğ° Ğ†Ğ²Ğ°Ğ½ Ğ¤Ñ€Ğ°Ğ½ĞºĞ¾ Ğ±ÑƒĞ»Ğ¸ Ğ²ĞµĞ»Ğ¸ĞºĞ¸Ğ¼Ğ¸ 
        Ğ¿Ğ¸ÑÑŒĞ¼ĞµĞ½Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸. ĞĞ°ÑƒĞºĞ¾Ğ²Ğ¾-Ñ‚ĞµÑ…Ğ½Ñ–Ñ‡Ğ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑ Ğ·Ğ°Ğ±ĞµĞ·Ğ¿ĞµÑ‡ÑƒÑ” Ñ€Ğ¾Ğ·Ğ²Ğ¸Ñ‚Ğ¾Ğº ÑÑƒÑĞ¿Ñ–Ğ»ÑŒÑÑ‚Ğ²Ğ°.
        """
        
        print("ğŸ‡ºğŸ‡¦ Ukrainian Text Processing:")
        chunks = await self.processor.process_text(ukrainian_text, "ukrainian_test.txt")
        combined_text = " ".join(chunk.content for chunk in chunks)
        
        # Check preservation of compound terms
        preserved_terms = [
            "Ğ´ĞµÑ€Ğ¶Ğ°Ğ²Ğ½Ğ¾-Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğµ",
            "Ñ–Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ñ–Ğ¹Ğ½Ğ¾-ĞºĞ¾Ğ¼ÑƒĞ½Ñ–ĞºĞ°Ñ†Ñ–Ğ¹Ğ½Ğ¸Ñ…", 
            "Ğ½Ğ°ÑƒĞºĞ¾Ğ²Ğ¾-Ñ‚ĞµÑ…Ğ½Ñ–Ñ‡Ğ½Ğ¸Ğ¹"
        ]
        
        for term in preserved_terms:
            if term in combined_text.lower():
                print(f"   âœ… Preserved compound term: {term}")
            else:
                print(f"   âŒ Lost compound term: {term}")
        
        # Check name preservation
        if "Ñ‚Ğ°Ñ€Ğ°Ñ ÑˆĞµĞ²Ñ‡ĞµĞ½ĞºĞ¾" in combined_text.lower():
            print("   âœ… Preserved Ukrainian names")
        
        # English technical terms preservation
        english_text = """
        Machine Learning and Artificial Intelligence are transforming Natural Language Processing.
        Companies like Google, Microsoft, and OpenAI are leading the development of Large Language Models.
        The United States and European Union are investing heavily in AI research.
        """
        
        print("\nğŸ‡ºğŸ‡¸ English Text Processing:")
        chunks = await self.processor.process_text(english_text, "english_test.txt")
        combined_text = " ".join(chunk.content for chunk in chunks)
        
        technical_terms = [
            "Machine Learning",
            "Artificial Intelligence", 
            "Natural Language Processing",
            "Large Language Models"
        ]
        
        for term in technical_terms:
            if term in combined_text:
                print(f"   âœ… Preserved technical term: {term}")
        
        proper_nouns = ["Google", "Microsoft", "OpenAI", "United States"]
        for noun in proper_nouns:
            if noun in combined_text:
                print(f"   âœ… Preserved proper noun: {noun}")

    async def demo_file_processing(self):
        """Demo file processing capabilities"""
        print("\nğŸ“ FILE PROCESSING DEMO")
        print("=" * 50)
        
        data_dir = Path(__file__).parent.parent / "data"
        
        test_files = [
            ("sample_ua.txt", "Ukrainian Document"),
            ("sample_en.txt", "English Document"), 
            ("sample_zh.txt", "Chinese Document"),
            ("sample_code.py", "Multilingual Code File")
        ]
        
        for filename, description in test_files:
            file_path = data_dir / filename
            
            if file_path.exists():
                print(f"\nğŸ“„ Processing {description}:")
                print(f"   File: {filename}")
                
                start_time = time.time()
                chunks = await self.processor.process_file(file_path)
                processing_time = time.time() - start_time
                
                if chunks:
                    stats = self.processor.get_statistics(chunks)
                    
                    print(f"   ğŸŒ Languages: {list(stats['languages'].keys())}")
                    print(f"   ğŸ”¢ Total Chunks: {stats['total_chunks']}")
                    print(f"   ğŸ“Š Avg Confidence: {stats['average_confidence']}")
                    print(f"   â±ï¸  Processing Time: {processing_time:.3f}s")
                    print(f"   ğŸ“ Size Range: {stats['chunk_size_stats']['min']}-{stats['chunk_size_stats']['max']} chars")
                else:
                    print("   âŒ No chunks created")
            else:
                print(f"\nğŸ“„ {description}: File not found ({filename})")

    async def demo_performance_benchmarks(self):
        """Demo performance with various text sizes"""
        print("\nâš¡ PERFORMANCE BENCHMARKS")
        print("=" * 50)
        
        test_sizes = [
            (100, "Small Text (100 chars)"),
            (1000, "Medium Text (1KB)"),
            (10000, "Large Text (10KB)"),
            (50000, "Very Large Text (50KB)")
        ]
        
        base_text = "This is a performance test sentence for multilingual document processing. " \
                   "It contains technical terms like Machine Learning and Artificial Intelligence. "
        
        for size, description in test_sizes:
            # Generate text of specified size
            repeat_count = max(1, size // len(base_text))
            test_text = base_text * repeat_count
            actual_size = len(test_text)
            
            print(f"\nğŸ”¬ {description}")
            print(f"   Actual Size: {actual_size:,} characters")
            
            # Benchmark processing
            start_time = time.time()
            chunks = await self.processor.process_text(test_text, f"perf_test_{size}.txt")
            processing_time = time.time() - start_time
            
            # Calculate metrics
            chars_per_second = actual_size / processing_time if processing_time > 0 else 0
            chunks_per_second = len(chunks) / processing_time if processing_time > 0 else 0
            
            print(f"   â±ï¸  Processing Time: {processing_time:.3f}s")
            print(f"   ğŸš€ Speed: {chars_per_second:,.0f} chars/sec")
            print(f"   ğŸ”¢ Chunks Created: {len(chunks)} ({chunks_per_second:.1f} chunks/sec)")
            
            if chunks:
                avg_chunk_size = sum(len(c.content) for c in chunks) / len(chunks)
                print(f"   ğŸ“ Average Chunk Size: {avg_chunk_size:.0f} characters")

    async def demo_multilingual_statistics(self):
        """Demo comprehensive statistics"""
        print("\nğŸ“Š MULTILINGUAL STATISTICS DEMO")
        print("=" * 50)
        
        # Process multiple documents in different languages
        all_chunks = []
        
        multilingual_texts = {
            "en": "English technical documentation about Artificial Intelligence and Machine Learning systems.",
            "uk": "Ğ£ĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ° Ñ‚ĞµÑ…Ğ½Ñ–Ñ‡Ğ½Ğ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ñ–Ñ Ğ¿Ñ€Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¸ ÑˆÑ‚ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ–Ğ½Ñ‚ĞµĞ»ĞµĞºÑ‚Ñƒ Ñ‚Ğ° Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ²Ñ‡Ğ°Ğ½Ğ½Ñ.",
            "zh": "å…³äºäººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ ç³»ç»Ÿçš„ä¸­æ–‡æŠ€æœ¯æ–‡æ¡£ã€‚",
            "ru": "Ğ ÑƒÑÑĞºĞ°Ñ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.",
            "es": "DocumentaciÃ³n tÃ©cnica en espaÃ±ol sobre sistemas de Inteligencia Artificial y Machine Learning."
        }
        
        for lang_code, text in multilingual_texts.items():
            chunks = await self.processor.process_text(text * 3, f"demo_{lang_code}.txt")  # Repeat for more content
            all_chunks.extend(chunks)
        
        # Generate comprehensive statistics
        stats = self.processor.get_statistics(all_chunks)
        
        print("\nğŸ“ˆ Overall Statistics:")
        print(f"   Total Documents: {len(multilingual_texts)}")
        print(f"   Total Chunks: {stats['total_chunks']}")
        print(f"   Total Characters: {stats['total_characters']:,}")
        print(f"   Total Words: {stats['total_words']:,}")
        print(f"   Average Confidence: {stats['average_confidence']}")
        
        print("\nğŸŒ Language Distribution:")
        for lang, count in stats['languages'].items():
            percentage = (count / stats['total_chunks']) * 100
            print(f"   {lang}: {count} chunks ({percentage:.1f}%)")
        
        print(f"\nğŸ”§ Supported Languages: {len(stats['supported_languages'])}")
        print(f"   Languages: {', '.join(stats['supported_languages'])}")

    async def demo_integration_example(self):
        """Demo integration with embedding service (simulated)"""
        print("\nğŸ”— INTEGRATION EXAMPLE")
        print("=" * 50)
        
        text = """
        ISKALA MOVA is a Ukrainian AI system with multilingual capabilities.
        Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ†Ğ¡ĞšĞĞ›Ğ ĞœĞĞ’Ğ Ğ¿Ñ–Ğ´Ñ‚Ñ€Ğ¸Ğ¼ÑƒÑ” Ğ±Ğ°Ğ³Ğ°Ñ‚Ğ¾Ğ¼Ğ¾Ğ²Ğ½Ñƒ Ğ¾Ğ±Ñ€Ğ¾Ğ±ĞºÑƒ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ–Ğ².
        è¯¥ç³»ç»Ÿæ”¯æŒå¤šè¯­è¨€æ–‡æ¡£å¤„ç†åŠŸèƒ½ã€‚
        """
        
        print("Processing multilingual text...")
        chunks = await self.processor.process_text(text, "integration_demo.txt")
        
        print(f"\nğŸ“‹ Generated {len(chunks)} chunks for embedding:")
        
        for i, chunk in enumerate(chunks):
            print(f"\nğŸ”¸ Chunk {i+1}:")
            print(f"   Language: {chunk.language}")
            print(f"   Content: {chunk.content[:100]}...")
            print(f"   Confidence: {chunk.confidence:.3f}")
            print(f"   Words: {chunk.word_count}")
            print(f"   Hash: {chunk.chunk_hash}")
            
            # Simulate embedding integration
            print(f"   ğŸ§  Ready for EmbeddingService.get_embedding('{chunk.chunk_id}')")

    async def run_full_demo(self):
        """Run complete demonstration"""
        print("ğŸŒ ISKALA MOVA MULTILINGUAL DOCUMENT PROCESSOR DEMO")
        print("=" * 60)
        print("Enterprise-grade document processing with 50+ language support")
        print("=" * 60)
        
        demos = [
            ("Language Detection", self.demo_language_detection),
            ("Tokenizer Features", self.demo_tokenizer_features),
            ("File Processing", self.demo_file_processing),
            ("Performance Benchmarks", self.demo_performance_benchmarks),
            ("Multilingual Statistics", self.demo_multilingual_statistics),
            ("Integration Example", self.demo_integration_example)
        ]
        
        for name, demo_func in demos:
            try:
                await demo_func()
                print(f"\nâœ… {name} demo completed successfully")
            except Exception as e:
                print(f"\nâŒ {name} demo failed: {e}")
        
        print("\nğŸ‰ DEMO COMPLETED!")
        print("=" * 60)
        print("ğŸš€ ISKALA MOVA MultilingualDocumentProcessor is ready for production!")

async def main():
    """Main demo execution"""
    demo = MultilingualDemo()
    await demo.run_full_demo()

if __name__ == "__main__":
    # Run the complete demo
    asyncio.run(main()) 